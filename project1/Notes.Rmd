---
title: "Presentation"
output: 
  html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{r}
library(tidyverse)
library(kableExtra)
library(factoextra)

set.seed(10)
```

TODO: DodaÄ‡ plot dla k =3

K-means is one of the most popular clustering techniques, used by many for a long time. Its simplicity and relative speed are appealing. However, the corectness of the results can be questionable and vary from one run to another. 

Let's create a simple dataset to demonstrate that. I have created 3 clusters in 2-dimensional space. Each cluster was obtained by sampling 100 points from gaussian distribution, with sd=1. I have arbitrarly chosen cluster centers. As I will repeat the procedure of generating random clusters throughout the whole analysis, I have created simple functions to automate this task.
First function, *create_one_cluster*, creates points from one cluster when given center:

```{r }
create_one_cluster <- function(center, size) {
  # Center - cluster center coordinates, given as list
  # Size - amount of points to create
  
  center %>% purrr::map(function(x) {
    rnorm(size, mean = x, sd = 1)
  }) %>%
    as.data.frame() -> out
  
  names(out) <- paste0("x", seq_along(names(out)))
  out
}
```


Results look like this:

```{r}
create_one_cluster(list(1,2), 10) %>% 
  kable() %>% 
  kable_styling()
```


This function takes a list of cluster centers and generates points:

```{r}
create_clusters_from_centers <- function(centers, size = 10) {
  # centers - list
  centers %>%
    purrr::map(create_one_cluster, size) %>%
    dplyr::bind_rows(.id = "cluster_id")
}
```


```{r}
cluster_centers <- list(c(1, 10),
                        c(2, 3),
                        c(5, 8))

create_clusters_from_centers(cluster_centers, 100) -> data_with_id
```


```{r}
data_with_id %>%
  head() %>%
  kable() %>% 
  kable_styling()
```

Let's plot the dataset:

```{r fig.width=10}
ggplot(data_with_id, aes(x = x1, y = x2, color = cluster_id)) +
  geom_point() +
  theme_minimal(base_size = 13) +
  scale_color_brewer(palette = "Set1") +
  coord_fixed() +
  labs(color = "Cluster") +
  scale_x_continuous(breaks = seq(0, 12, 2)) +
  scale_y_continuous(breaks = seq(0, 12, 2))
```

This looks like a pretty clusterable dataset. Indeed, running kmeans give a nice, predicted result:

```{r fig.width=10}
data <- data_with_id %>% select(-cluster_id)

kmeans(data, centers = 3) -> res
res %>% 
  fviz_cluster(data, geom = "point") +
  theme_minimal(base_size = 13) +
  scale_color_brewer(palette = "Set1") +
  coord_fixed() +
  labs(title = NULL)

```

The sum of between sum of squares is `r round(res$betweenss,2)`.

Now, let's run the algorithm multiple times, and check how this metric is going to behave. Again, for convenience I have created a function gathering results from multiple runs automatically. The function uses *broom* to tidy the results in a tidyverse-way:

```{r}
run <- function(data,
                niter = 1000,
                k = 3) {
  # Bulk standard k means clustering
  map(1:niter, function(x) {
      kmeans(data, centers = k)
  }) -> bulk_results
  
  # saving cluster results to data frame
  bulk_results %>%
    map_df(function(x) {
      broom::glance(x)
    }) -> cluster_results
  
  cluster_results$cluster_object <- bulk_results
  
  
  cluster_results
}

```

```{r}
bulk_results <- run(data, niter = 1000, k = 3)
bulk_results %>% 
  head(5)

```


Let's check how the total within sum of squares behaved.
```{r fig.width=10}
ggplot(bulk_results, aes(x=tot.withinss))+ 
  geom_histogram(fill = "#377EB8") +
  theme_minimal(base_size = 13) +
  coord_fixed()
```

As we can see, the majority of clusterings is close to the most optimal one. However, there is also some amount of clusterings with betweenss at around 2650. This is how such example clustering looks like:

```{r fig.width=10}
bulk_results %>%
  filter(tot.withinss < 1000) %>%
  head(1) %>%
  .$cluster_object %>%
  .[[1]] -> bad_clustering

  fviz_cluster(bad_clustering, data = data, geom = "point") +
    theme_minimal(base_size = 13) +
    scale_color_brewer(palette = "Set1") +
    coord_fixed() +
    labs(title = NULL)
```


As we can see, clustering is highly suboptimal compared to ground-truth model. 

One could wonder, how to overcome these fault clusterings. One way would be to run the algorithm multiple times, and choose the best run. The solution was proposed by ... . The main part of algoirthm stays the same. The difference lies in centers initialisation phase. In standard k-means, k centers are randomly chosen from the input dataset. In the original kmeans++ paper, the algorithm was defined as follows: ...



Intuition behind k-means++ is as follows: Choosing centers distant from each other is a good thing. Authors prove that this algoirthm asymptotically gives ln k better results than random initialisation algorithm. 


In python Machine Learning library *scikit-learn*, default k-means algorithm actually implements "++" version by default. 
In R, k-means++ is implemented in *flexclust* package. To test both approaches, however, was inconvenient, as *flexclust* is implemented in S4 classes, and that makes it incompatibile with other clustering packages, factoextra for instance. Thats why I have copied inner workings of algorithm from flexclust, which is defined in private function kmeanspp. I have also optimised the function to take advantage of vectorised operations. This gave a 2-times time improvement. The improved algorithm looks like this:

```{r}
kmeanspp <- function(x, k)
{
  x <- as.matrix(x)
  centers <- matrix(0, nrow = k, ncol = ncol(x))
  centers[1, ] <- x[sample(1:nrow(x), 1), , drop = FALSE] # Take first point at random
  target_point <- centers[1L, , drop = FALSE] 
  
  map(1:ncol(x), function(i){
      (x[,i] - target_point[,i]) ^ 2
    }) -> distances_list
    
    
  d <- sqrt(Reduce(`+`, distances_list))^2
  
  for (l in 2:k) {
    centers[l, ] <- x[sample(1:nrow(x), 1, prob = d), , drop = FALSE] # Select next point with probability weighted by distance from starting point
    
    target_point <- centers[l, , drop = FALSE]
    
    map(1:ncol(x), function(i){
      (x[,i] - target_point[,i]) ^ 2
    }) -> distances_list
    
    
    arg2 <- sqrt(Reduce(`+`, distances_list))^2
    d <- pmin(d, arg2)
    
    
    
    
    
  }
  centers
}
```


To apply it in replacement for stats::kmeans default, the code looks like this:

```{r}
kmeans(data, centers = kmeanspp(data, 3))
```





To test the working of the algorithm, I have applied standard and "++" version in few conditions. 
I have created few artificial datasets and manipulated few parameters, namely number of clusters *k* and number of dimensions *dim_cnt*. 

I have created two artificial datasets - one with k=3 and 2 dimensions, and the other one with k=10 and 10 dimensions. In both cases, cluster centers were chosen uniformly from a [0, 500] space. The clusters were generated from n-dimensional Gaussian with cov=0 and sd = 1. Each cluster consisted of 100 points. 

As the dataset is artificially generated, we explicitly know what perfect centers are. To use this fact, I have run kmeans with perfect centers passed. This way we can compare results of clusterings to a perfect benchmark. The code:

```{r}
load("results/batch_experiments.Rdata")

results_k10_n10_perfect <- kmeans(data_k10_n10, centers = data_k10_n10_centers)
results_k3_n2_perfect <- kmeans(data_k3_n2, centers = data_k3_n2_centers)

```

Value of tot.withinss for k = 3 is equal to `r round(results_k3_n2_perfect$tot.withinss, 2)`, while for dataset with k = 10 - `r round(results_k10_n10_perfect$tot.withinss, 2)`



```{r}
d<-get_dist(data_k10_n10[sample(1:nrow(data_k10_n10), 100),], method = "euclidean")
fviz_dist(d, show_labels = FALSE)+ 
  labs(title = "Dissimilarity matrix",
       subtitle = "(n = 10, k= 10)") -> diss_k10_n10

d<-get_dist(data_k3_n2, method = "euclidean")
fviz_dist(d, show_labels = FALSE)+ 
  labs(
    title = NULL,
    subtitle = "(n = 2, k= 3)") -> diss_k3_n2
```

```{r fig.width=10, fig.height=5}
cowplot::plot_grid(diss_k10_n10, diss_k3_n2)
```


As the simulations took a long time on my computer, I calculated them once and saved as Rdata. The codes are avaliable in *aa.R* here. I have plotted total within-sum-of-squares. Once again, the less the better:

Dashed line presents benchmark results, which were caluclated earlier. 

```{r fig.width=10}

results_k10_n10 %>% 
  mutate(type = "standard") %>%
  rbind(results_k10_n10_pp %>% 
          mutate(type = "pp")) %>%
  mutate(type = factor(type, levels = c("standard", "pp"))) -> results_k10_n10_all

ggplot(results_k10_n10_all, aes(x = tot.withinss, fill = type, color = type)) +
         geom_histogram(alpha=0.5) +
  geom_vline(xintercept =  round(results_k10_n10_perfect$tot.withinss, 2),
             color = "red", linetype = "dashed") +
  facet_grid(rows = vars(type), scales = "free") +
  theme_minimal(base_size = 13) +
  scale_color_brewer(palette = "Set1") +
  scale_fill_brewer(palette = "Set1") +
  labs(fill = "Algorithm",
       x = "tot.withinss (x 100000)") +
  guides(color = FALSE) +
  scale_x_continuous(labels = function(x) round(x/100000))

```

This picture says more than thousand words. K-means++ outperformed standard version substantially. Here are the results statistics:

```{r}
results_k10_n10_all %>% 
  select(-cluster_object) %>%
  group_by(type) %>%
  summarise(
    min = min(tot.withinss),
    q25 = quantile(tot.withinss, 0.25),
    q50 = quantile(tot.withinss, 0.5),
    q75 = quantile(tot.withinss, 0.75),
    max = max(tot.withinss)
  ) %>% 
  arrange(desc(type)) %>%
  kable() %>%
  kable_styling()
```



What are the drawbacks of kmeans++? There is one: speed. As of standard algorithm, the initialisation phase is done in consant time - one should only choose k random points from the dataset. In kmeans++, on the contrary, after the first center is chosen, the distance to each of the remaining points are calculated. This means passing the data k times, thus time complexity is *O(k*n)*. I have created a basic benchmark for both algorithms comparison:
```{r}
standard <- function(){kmeans(data, centers = 3)}
pp <- function(){kmeans(data, centers = kmeanspp(data, 3))}
microbenchmark::microbenchmark(
standard(),
pp()
)

```


As we can see, the standard function is roughly 2 times faster than kmeans++. For some applications, this disqualifies the method. There are however possible workarounds. One thing is alogirthm complexity itself, and the other is implementation - to make kmeans++ code faster, implementing it in C or Fortran (as basic *stats::kmeans* code) would be needed.

There is also other proposed solution made by ... , aimed specifically at big data solutions. The authors show a k-means|| (parallel) algoirthm. It has the same theoretical improvements as kmeans++, however is substantially faster, and can be implemented to leverage multiple threads. 





